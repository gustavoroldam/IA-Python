# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wqlwpDmiTsKVLcBp3XzubS39_yTxZisB
"""

import pandas as pd
import re
import nltk
nltk.download('stopwords')
nltk.download('wordnet')
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB

data_treinamento = pd.read_csv('base_dados_treinamento.csv', sep = ';', encoding='utf-8', on_bad_lines='skip')

data_treinamento.head()

data_treinamento['sentimento'].value_counts()

data_aplicacao = pd.read_csv('base_dados_analise.csv', sep=';', encoding='utf-8', on_bad_lines='skip')
data_aplicacao.head()

#Configurando os textos de Base
def to_lower_case(texto):
  return texto.lower()

def limpar_texto(texto):
  texto = re.sub(r'http\S+', '', texto).replace('.', '')
  texto = re.sub(r'^.+@[^\.].*\.[a-z]{2,}$', 'texto-email', texto)
  texto = re.sub(r'.\$', 'valor-monetario', texto)
  texto = re.sub(r'\d(\.|,\d+)?', 'valor-numerico', texto)
  return texto


from nltk.corpus import stopwords
from string import punctuation
stopwords_custom = set(stopwords.words('portuguese') + list(punctuation))
stopwords_custom.remove('não')


def remover_stopwords(texto):
  palavras = [i for i in texto.split() if not i in stopwords_custom]
  return (" ".join(palavras))

def pre_processar_dados(texto):
  texto = to_lower_case(texto)
  texto = limpar_texto(texto)
  texto = remover_stopwords(texto)
  return texto

textos_treinamento = []
for texto in data_treinamento['texto']:
  #print('Antes: '+texto)
  texto_pre_processado = pre_processar_dados(texto)
  #print('Depois: '+texto_pre_processado)
  textos_treinamento.append(texto_pre_processado)

classificacao_treinamento = data_treinamento['sentimento']

textos_aplicacao = []
for texto in data_aplicacao['Feedback']:
  #print('Antes: '+texto)
  texto_pre_processado = pre_processar_dados(texto)
  #print('Depois: '+texto_pre_processado)
  textos_aplicacao.append(texto_pre_processado)

from nltk.tokenize import TweetTokenizer

text_tokenizer = TweetTokenizer()
vetorizador = CountVectorizer(analyzer='word', tokenizer=text_tokenizer.tokenize)

frequencia_treinamento = vetorizador.fit_transform(textos_treinamento)
#print(frequencia_treinamento)

frequencia_aplicacao = vetorizador.transform(textos_aplicacao)
#print(frequencia_aplicacao)

ModeloIA = MultinomialNB()
ModeloIA.fit(frequencia_treinamento, classificacao_treinamento)

modelo_aplicacao = ModeloIA.predict(frequencia_aplicacao)
textos_analizados = zip(textos_aplicacao, modelo_aplicacao)

resultados = []
for texto, classificacao_analizada in textos_analizados:
  resultado = {
      "Textos": texto,
      "Classificação": classificacao_analizada
  }
  resultados.append(resultado)

dataframe_resultado = pd.DataFrame(resultados)
dataframe_resultado